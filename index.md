# Welcome to the Resources Page
Here are some useful resources. I will keeep updating this page throughout my learning journey.

## üìö Resources
- [Umar Jamil - LLM Videos](https://umarjamil.org/videos)
- [Lena Voita - NLP Course | For You](https://lena-voita.github.io/nlp_course.html)


## üï∏Ô∏è Refresher on Neural Networks
- [3Blue1Brown (Youtube) - But what is a neural network? | Deep learning chapter 1](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)


## üó£Ô∏è Language Modeling
- [Lena Voita - Language Modeling](https://lena-voita.github.io/nlp_course/language_modeling.html)


## üî° Word Embeddings
- [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html)
- [IBM - What Are Word Embeddings?](https://www.ibm.com/think/topics/word-embeddings)


## üîÑ Recurrent Neural Networks (RNNs)
- [StatQuest with Josh Starmer (Youtube) - Recurrent Neural Networks (RNNs)](https://www.youtube.com/watch?v=AsNTP8Kwu80)
- [NLP from Scratch ‚Äî PyTorch Tutorials 2.5.0+cu124 documentation](https://pytorch.org/tutorials/intermediate/nlp_from_scratch_index.html)
- [Building a Recurrent Neural Network From Scratch | by Long Nguyen | Medium](https://medium.com/@thisislong/building-a-recurrent-neural-network-from-scratch-ba9b27a42856)


## ‚åõ Long Short-Term Memory (LSTMs)
- [StatQuest with Josh Starmer (Youtube) - Long Short-Term Memory (LSTM)](https://www.youtube.com/watch?v=YCzL96nL7j0)
- [Building a LSTM by hand on PyTorch | by Piero Esposito | Towards Data Science](https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091)


## üîÅ Seq2Seq (Encoder-Decoder)
- [Lena Voita - Seq2seq and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
- [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention ‚Äî PyTorch Tutorials 2.5.0+cu124 documentation](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)


## üß± Transformer
- [Umar Jamil (Youtube) - Attention is all you need (Transformer) - Model explanation (including math), Inference and Training (Youtube)](https://www.youtube.com/watch?v=bCz4OMemCcA)
  - `Positional Encoding`, `Self-Attention`, `Multi-head Attention`, `Masked Multi-head Attention`, `Layer Normalization`, `Inference Strategies`
- [Umar Jamil (Youtube) - Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.](https://www.youtube.com/watch?v=ISNdQcPhsts)
- [Transformer Explainer: LLM Transformer Model Visually Explained](https://poloclub.github.io/transformer-explainer/)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Aman's AI Journal ‚Ä¢ Primers ‚Ä¢ Transformers](https://aman.ai/primers/ai/transformers/#enter-the-transformer)


## üë® Encoder-Only Models
- [Umar Jamil (Youtube) - BERT explained: Training, Inference,  BERT vs GPT/LLamA, Fine tuning, [CLS] token](https://www.youtube.com/watch?v=90mGPxR2GgY)
  - `Masked Language Modeling (MLM)`, `Next Sentence Prediction (NSP)`, `[CLS] Token`
- [Mastering BERT Model: Building it from Scratch with Pytorch | by CheeKean | Data And Beyond | Medium](https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891)


## ü¶ô Decoder-Only Models
- [Umar Jamil (Youtube) - LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo)
  - `RMS Normalization`, `KV Cache`, `Rotary Positional Encoding`, `SwiGLU`, `Internal Covariate Shift`
- [Umar Jamil (Youtube) - Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm](https://www.youtube.com/watch?v=oM4VmoabDAI)
- [Andrej Karpathy (Youtube) - Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)


## üì± Multimodal Large Language Models	
- [ Sebastian Raschka - Understanding Multimodal LLMs](https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html) 
- [Umar Jamil (Youtube) - Coding a Multimodal (Vision) Language Model from scratch in PyTorch with full explanation](https://www.youtube.com/watch?v=vAmKB7iPkWw)


## üîé Further Knowledge & Additional Terms
### üóÉÔ∏è Retrieval Augmented Generation (RAG)
- [Umar Jamil (Youtube) - Retrieval Augmented Generation (RAG) Explained: Embedding, Sentence BERT, Vector Database (HNSW)](https://www.youtube.com/watch?v=rhZgXNdhWDY)
  - `SentenceBERT`, `Mean Pooling`, `VectorDB`, `Hierarchical Navigable Small Worlds`

### üè∑Ô∏è Positional Encoding
- [Positional Encoding Explained: A Deep Dive into Transformer PE](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b) 
- [Understanding Rotary Positional Encoding | by Ngieng Kianyew | Medium](https://medium.com/@ngiengkianyew/understanding-rotary-positional-encoding-40635a4d078e)
	
### üéüÔ∏è Tokenizers
- [Byte-Pair Encoding tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/5?fw=pt)
- [WordPiece tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/6?fw=pt)
- [Unigram tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/7?fw=pt)

### üñ®Ô∏è Decoding Methods for Text Generation
- [Hugging Face Blog How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)
- [Decoding Strategies in Language Models | Rajan Ghimire](https://r4j4n.github.io/blogs/posts/text_decoding/)

### üßÆ Parameter-Efficient Tuning
- [LoRA: Low-Rank Adaptation of Large Language Models - Explained visually + PyTorch code from scratch](https://www.youtube.com/watch?v=PXWYUTMt-AU)
